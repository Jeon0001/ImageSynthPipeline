{"cells":[{"cell_type":"markdown","id":"a1ae39ff","metadata":{"id":"a1ae39ff"},"source":["# Object masks from prompts with SAM"]},{"cell_type":"markdown","id":"b4a4b25c","metadata":{"id":"b4a4b25c"},"source":["\n","The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt.\n"]},{"cell_type":"markdown","id":"644532a8","metadata":{"id":"644532a8"},"source":["## Environment Set-up"]},{"cell_type":"code","execution_count":1,"id":"6aL7AQn2E5PS","metadata":{"executionInfo":{"elapsed":6296,"status":"ok","timestamp":1737895830805,"user":{"displayName":"Kyaw Ye Thu","userId":"06739278510030916737"},"user_tz":-540},"id":"6aL7AQn2E5PS"},"outputs":[],"source":["import torch\n","import numpy as np\n","import IPython\n","import matplotlib.pyplot as plt\n","import cv2\n","import io\n","import PIL.Image as Image\n","import os\n","import sys\n","from torchvision.transforms import GaussianBlur\n","from tqdm import tqdm\n","\n","IN_COLAB = 'google.colab' in sys.modules"]},{"cell_type":"code","execution_count":2,"id":"91dd9a89","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":11652,"status":"ok","timestamp":1737895842454,"user":{"displayName":"Kyaw Ye Thu","userId":"06739278510030916737"},"user_tz":-540},"id":"91dd9a89","outputId":"9f809936-7d37-4554-f87f-cb65b1d1fa51"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA is available: True\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n","Collecting git+https://github.com/facebookresearch/segment-anything.git\n","  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-1ga40ceq\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-1ga40ceq\n","  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ipympl in /usr/local/lib/python3.11/dist-packages (0.9.6)\n","Requirement already satisfied: ipython<9 in /usr/local/lib/python3.11/dist-packages (from ipympl) (7.34.0)\n","Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.11/dist-packages (from ipympl) (7.7.1)\n","Requirement already satisfied: matplotlib<4,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from ipympl) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ipympl) (1.26.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from ipympl) (10.4.0)\n","Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.11/dist-packages (from ipympl) (5.7.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (75.1.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (3.0.50)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (4.9.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (5.5.6)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.6.10)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (4.55.4)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (2.8.2)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.1.12)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.3.3)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<9->ipympl) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4,>=3.5.0->ipympl) (1.17.0)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.5.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.1.5)\n","Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (24.0.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (23.1.0)\n","Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (5.7.2)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (5.10.4)\n","Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (7.16.5)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.6.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.21.1)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.2.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.3.6)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.2.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.12.3)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.2.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.3.0)\n","Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.1.5)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.1.0)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.10.2)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.5.1)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.21.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.23.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (21.2.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.5.1)\n","Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.4.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (24.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.36.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.22.3)\n","Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.24.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.17.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.22)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.0)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.3.1)\n"]}],"source":["print(\"CUDA is available:\", torch.cuda.is_available())\n","\n","\n","!{sys.executable} -m pip install opencv-python\n","!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n","!pip install ipympl\n","\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","face_detection_model = \"face_detection_yunet_2023mar.onnx\"\n","\n","if not os.path.isdir('models'):\n","  !mkdir models\n","# download SAM\n","  if not os.path.isfile(f'models/{sam_checkpoint}'):\n","    !wget https://dl.fbaipublicfiles.com/segment_anything/$sam_checkpoint\n","    !mv $sam_checkpoint models/\n","\n","  # download YuNet Faec Detection Model\n","  if not os.path.isfile(f'models/{face_detection_model}'):\n","    !wget https://github.com/astaileyyoung/CineFace/raw/main/research/data/face_detection_yunet_2023mar.onnx\n","    !mv $face_detection_model models/\n","\n","if not os.path.isdir('images/original_images'):\n","  !mkdir -p images/original_images\n","\n","if not os.path.isdir('images/masks'):\n","  !mkdir -p images/masks"]},{"cell_type":"markdown","id":"0be845da","metadata":{"id":"0be845da"},"source":["## Program Set-up"]},{"cell_type":"markdown","id":"0bb1927b","metadata":{"id":"0bb1927b"},"source":["Load the SAM model and predictor. Running on CUDA and using the default model are recommended for best results."]},{"cell_type":"code","execution_count":3,"id":"7e28150b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25321,"status":"ok","timestamp":1737895867771,"user":{"displayName":"Kyaw Ye Thu","userId":"06739278510030916737"},"user_tz":-540},"id":"7e28150b","outputId":"5c1dc347-f42f-427f-e75a-25821c079e85"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(f)\n"]}],"source":["from segment_anything import sam_model_registry, SamPredictor\n","\n","sys.path.append(\"..\")\n","\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=f'models/{sam_checkpoint}')\n","sam.to(device=device)\n","\n","predictor = SamPredictor(sam)"]},{"cell_type":"markdown","id":"33681dd1","metadata":{"id":"33681dd1"},"source":["Face detector class, `FaceDetectorYuNet()`, and helper functions for displaying points, boxes, and masks are defined."]},{"cell_type":"code","execution_count":4,"id":"29bc90d5","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1737895867772,"user":{"displayName":"Kyaw Ye Thu","userId":"06739278510030916737"},"user_tz":-540},"id":"29bc90d5"},"outputs":[],"source":["class FaceDetectorYunet():\n","    def __init__(self,\n","                  model_path='models/face_detection_yunet_2023mar.onnx',\n","                  img_size=(300, 300),\n","                  threshold=0.5):\n","        self.model_path = model_path\n","        self.img_size = img_size\n","        self.fd = cv2.FaceDetectorYN_create(str(model_path),\n","                                            \"\",\n","                                            img_size,\n","                                            score_threshold=threshold)\n","\n","    def draw_faces(self, image, faces, show_confidence=False):\n","        for face in faces:\n","            color = (0, 0, 255)\n","            thickness = 2\n","            cv2.rectangle(image, (face['x1'], face['y1']), (face['x2'], face['y2']), color, thickness, cv2.LINE_AA)\n","\n","            if show_confidence:\n","                confidence = face['confidence']\n","                confidence = \"{:.2f}\".format(confidence)\n","                position = (face['x1'], face['y1'] - 10)\n","                font = cv2.FONT_HERSHEY_SIMPLEX\n","                scale = 0.5\n","                thickness = 1\n","                cv2.putText(image, confidence, position, font, scale, color, thickness, cv2.LINE_AA)\n","        return image\n","\n","    def scale_coords(self, image, prediction):\n","        ih, iw = image.shape[:2]\n","        rw, rh = self.img_size\n","        a = np.array([\n","                (prediction['x1'], prediction['y1']),\n","                (prediction['x1'] + prediction['x2'], prediction['y1'] + prediction['y2'])\n","                    ])\n","        b = np.array([iw/rw, ih/rh])\n","        c = a * b\n","        prediction['img_width'] = iw\n","        prediction['img_height'] = ih\n","        prediction['x1'] = int(c[0,0].round())\n","        prediction['x2'] = int(c[1,0].round())\n","        prediction['y1'] = int(c[0,1].round())\n","        prediction['y2'] = int(c[1,1].round())\n","        prediction['face_width'] = (c[1,0] - c[0,0])\n","        prediction['face_height'] = (c[1,1] - c[0,1])\n","        # prediction['face_width'] = prediction['x2'] - prediction['x1']\n","        # prediction['face_height'] = prediction['y2'] - prediction['y1']\n","        prediction['area'] = prediction['face_width'] * prediction['face_height']\n","        prediction['pct_of_frame'] = prediction['area']/(prediction['img_width'] * prediction['img_height'])\n","        return prediction\n","\n","    def detect(self, image):\n","        if isinstance(image, str):\n","            image = cv2.imread(str(image))\n","        img = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n","        img = cv2.resize(img, self.img_size)\n","        self.fd.setInputSize(self.img_size)\n","        _, faces = self.fd.detect(img)\n","        if faces is None:\n","            return None\n","        else:\n","            predictions = self.parse_predictions(image, faces)\n","            return predictions\n","\n","    def parse_predictions(self,\n","                          image,\n","                          faces):\n","        data = []\n","        for num, face in enumerate(list(faces)):\n","            x1, y1, x2, y2 = list(map(int, face[:4]))\n","            landmarks = list(map(int, face[4:len(face)-1]))\n","            landmarks = np.array_split(landmarks, len(landmarks) / 2)\n","            positions = ['left_eye', 'right_eye', 'nose', 'right_mouth', 'left_mouth']\n","            landmarks = {positions[num]: x.tolist() for num, x in enumerate(landmarks)}\n","            confidence = face[-1]\n","            datum = {'x1': x1,\n","                     'y1': y1,\n","                     'x2': x2,\n","                     'y2': y2,\n","                     'face_num': num,\n","                     'landmarks': landmarks,\n","                     'confidence': confidence,\n","                     'model': 'yunet'}\n","            d = self.scale_coords(image, datum)\n","            data.append(d)\n","        return data\n","\n","\n","def show_mask(mask, ax, random_color=False, display=False):\n","    if random_color:\n","        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n","    else:\n","        # color to inpaint (white)\n","        color = np.array([255, 255, 255])\n","\n","    # color to keep intact (black)\n","    background_color = np.array([0, 0, 0])\n","    h, w = mask.shape[-2:]\n","\n","    # Reshape the mask to have the same number of color channels\n","    mask = mask.reshape(h, w, 1)\n","\n","    # Apply color to the mask where mask is True, and color2 where mask is False\n","    mask_image = mask * color.reshape(1, 1, -1) + ~mask * background_color.reshape(1, 1, -1)\n","    ax.imshow(mask_image)\n","\n","def show_points(coords, labels, ax, marker_size=375):\n","    pos_points = coords[labels==1]\n","    neg_points = coords[labels==0]\n","    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n","    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n","\n","def show_box(box, ax):\n","    x0, y0 = box[0], box[1]\n","    w, h = box[2] - box[0], box[3] - box[1]\n","    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"]},{"cell_type":"markdown","id":"kbwlMXRtn_tj","metadata":{"id":"kbwlMXRtn_tj"},"source":["## Batch Processing"]},{"cell_type":"markdown","id":"LSybFHjpIp-9","metadata":{"id":"LSybFHjpIp-9"},"source":["Extract the compressed file."]},{"cell_type":"code","execution_count":5,"id":"bSkoPbNQSPCo","metadata":{"id":"bSkoPbNQSPCo","executionInfo":{"status":"ok","timestamp":1737895566233,"user_tz":-540,"elapsed":1478,"user":{"displayName":"Kyaw Ye Thu","userId":"06739278510030916737"}}},"outputs":[],"source":["# !unzip ./food.zip -d ./images/input_images/\n","# !rm -rf ./food.zip\n","\n","!tar -xf original_images.tar.xz\n","!mv original_images/* images/original_images/\n","!rm -rf original_images/\n","!rm -rf ./original_images.tar.xz"]},{"cell_type":"code","source":["!rm -r images/original_images"],"metadata":{"id":"9R6MqcwBqglJ","executionInfo":{"status":"ok","timestamp":1737895977247,"user_tz":-540,"elapsed":465,"user":{"displayName":"Kyaw Ye Thu","userId":"06739278510030916737"}}},"id":"9R6MqcwBqglJ","execution_count":8,"outputs":[]},{"cell_type":"code","source":["!rm -r images/masks"],"metadata":{"id":"M2gTeQdRqkbS","executionInfo":{"status":"ok","timestamp":1737895984966,"user_tz":-540,"elapsed":458,"user":{"displayName":"Kyaw Ye Thu","userId":"06739278510030916737"}}},"id":"M2gTeQdRqkbS","execution_count":9,"outputs":[]},{"cell_type":"markdown","id":"l5qaMUUgU9M5","metadata":{"id":"l5qaMUUgU9M5"},"source":["Perform batch processing."]},{"cell_type":"code","execution_count":15,"id":"hnJTSkX_n2_t","metadata":{"collapsed":true,"id":"hnJTSkX_n2_t","executionInfo":{"status":"ok","timestamp":1737896225255,"user_tz":-540,"elapsed":535,"user":{"displayName":"Kyaw Ye Thu","userId":"06739278510030916737"}}},"outputs":[],"source":["def batch_detect_faces(input_img_dir, output_mask_dir, max_num_faces=2, save_boxes=False):\n","    # Create necessary directories if required\n","    if save_boxes: os.makedirs('images/box_images', exist_ok=True)\n","\n","    # Loop through all images in the directory\n","    for image_path in tqdm(os.listdir(input_img_dir)):\n","        full_image_path = os.path.join(input_img_dir, image_path)\n","\n","        # Skip non-image files (e.g., .DS_Store, other system files)\n","        if image_path.startswith('.') or not image_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n","            print(f\"Skipping non-image file: {image_path}\")\n","            continue\n","\n","        # Load the image\n","        img = cv2.imread(full_image_path)\n","\n","        # Check if the image was loaded successfully\n","        if img is None:\n","            print(f\"Failed to load image: {full_image_path}\")\n","            continue\n","\n","        faces = fd.detect(img)\n","\n","        # If no faces are detected, skip to the next image\n","        if not faces:\n","            print(f\"No face detected in {image_path}.\")\n","            continue\n","\n","        # Remove faces with low confidence\n","        for face in faces:\n","          if face['confidence'] < 0.55:\n","            faces.remove(face)\n","\n","        # Only select faces with max_num_faces highest confidence at most\n","        faces = sorted(faces, key=lambda x: x['confidence'], reverse=True)[:max_num_faces]\n","\n","        # If faces become empty after filtering\n","        if not faces:\n","            print(f\"No eligible faces detected in {image_path}.\")\n","            continue\n","\n","        # Prepare face center coordinates and labels\n","        face_centers = [((face['x1'] + face['x2']) / 2, (face['y1'] + face['y2']) / 2) for face in faces]\n","        input_point = np.array(face_centers)\n","        input_label = np.ones(len(input_point))\n","\n","        # Optionally save boxes around faces\n","        if save_boxes:\n","            box_file_name = os.path.splitext(os.path.basename(image_path))[0] + \"_box.png\"\n","            img_copy = img.copy()\n","            fd.draw_faces(img_copy, faces, show_confidence=True)\n","            cv2.imwrite(f'images/box_images/{box_file_name}', img_copy)\n","\n","        # Process image and make predictions\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        predictor.set_image(img)\n","        masks, scores, logits = predictor.predict(\n","            point_coords=input_point,\n","            point_labels=input_label,\n","            multimask_output=True,\n","        )\n","\n","        # Select the second mask, which usually includes just face and body parts\n","        mask_index = 1\n","        mask_input = logits[mask_index, :, :]\n","\n","        # Using the second mask, creates a more accurate mask\n","        masks, _, _ = predictor.predict(\n","            point_coords=input_point,\n","            point_labels=input_label,\n","            mask_input=mask_input[None, :, :],\n","            multimask_output=False,\n","        )\n","\n","        # Create a mask image\n","        mask_file_name = os.path.splitext(os.path.basename(image_path))[0] + \"_mask.png\"\n","        save_mask_path = os.path.join(output_mask_dir, mask_file_name)\n","        plt.figure(figsize=(10, 10))\n","        show_mask(masks, plt.gca())\n","        plt.axis('off')\n","\n","        # Save the mask with blur applied\n","        buf = io.BytesIO()\n","        plt.savefig(buf, format='png',bbox_inches='tight')\n","        buf.seek(0)\n","        plt.close()\n","        blur = GaussianBlur(kernel_size=23, sigma=20)\n","        blurred_mask = blur(Image.open(buf))\n","        blurred_mask.save(save_mask_path)\n"]},{"cell_type":"code","execution_count":16,"id":"1baFAavYsEXV","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3109,"status":"ok","timestamp":1737896231318,"user":{"displayName":"Kyaw Ye Thu","userId":"06739278510030916737"},"user_tz":-540},"id":"1baFAavYsEXV","outputId":"6522d637-4478-46ee-b3a8-eeade83609d0"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n"]}],"source":["fd = FaceDetectorYunet()\n","input_img_dir = '/content/images/original_images/'\n","output_mask_dir = '/content/images/masks/'\n","batch_detect_faces(input_img_dir, output_mask_dir)"]},{"cell_type":"code","execution_count":null,"id":"gIGc9aRPUkbs","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":367,"status":"ok","timestamp":1737872922275,"user":{"displayName":"Kyaw Ye Thu","userId":"03677166176861607126"},"user_tz":-540},"id":"gIGc9aRPUkbs","outputId":"0953608c-be95-457f-9d89-2c59fcdc356f"},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: content/images/masks/ (stored 0%)\n","  adding: content/images/masks/US_Festival_11_mask.png (deflated 9%)\n","  adding: content/images/masks/US_Festival_14_mask.png (deflated 20%)\n","  adding: content/images/masks/US_Festival_6_mask.png (deflated 11%)\n","  adding: content/images/masks/US_Festival_7_mask.png (deflated 25%)\n","  adding: content/images/masks/US_Festival_0_mask.png (deflated 2%)\n","  adding: content/images/masks/US_Festival_21_mask.png (deflated 13%)\n","  adding: content/images/masks/US_Festival_27_mask.png (deflated 10%)\n","  adding: content/images/masks/US_Festival_3_mask.png (deflated 27%)\n","  adding: content/images/masks/US_Festival_17_mask.png (deflated 3%)\n","  adding: content/images/masks/US_Festival_33_mask.png (deflated 10%)\n","  adding: content/images/masks/US_Festival_13_mask.png (deflated 13%)\n","  adding: content/images/masks/US_Festival_1_mask.png (deflated 17%)\n","  adding: content/images/masks/US_Festival_24_mask.png (deflated 5%)\n","  adding: content/images/masks/US_Festival_4_mask.png (deflated 8%)\n","  adding: content/images/masks/US_Festival_8_mask.png (deflated 8%)\n","  adding: content/images/masks/US_Festival_22_mask.png (deflated 20%)\n","  adding: content/images/masks/US_Festival_32_mask.png (deflated 24%)\n","  adding: content/images/masks/US_Festival_18_mask.png (deflated 9%)\n","  adding: content/images/masks/US_Festival_29_mask.png (deflated 12%)\n","  adding: content/images/masks/US_Festival_5_mask.png (deflated 38%)\n","  adding: content/images/masks/US_Festival_9_mask.png (deflated 11%)\n","  adding: content/images/masks/US_Festival_28_mask.png (deflated 9%)\n","  adding: content/images/masks/US_Festival_16_mask.png (deflated 10%)\n","  adding: content/images/masks/US_Festival_15_mask.png (deflated 1%)\n","  adding: content/images/masks/US_Festival_2_mask.png (deflated 11%)\n","  adding: content/images/masks/US_Festival_31_mask.png (deflated 12%)\n","  adding: content/images/masks/US_Festival_10_mask.png (deflated 12%)\n","  adding: content/images/masks/US_Festival_20_mask.png (deflated 5%)\n","  adding: content/images/masks/US_Festival_25_mask.png (deflated 4%)\n","  adding: content/images/masks/US_Festival_26_mask.png (deflated 7%)\n","  adding: content/images/masks/US_Festival_30_mask.png (deflated 8%)\n","  adding: content/images/masks/US_Festival_23_mask.png (deflated 3%)\n","  adding: content/images/masks/US_Festival_19_mask.png (deflated 29%)\n","  adding: content/images/masks/US_Festival_12_mask.png (deflated 20%)\n"]}],"source":["!zip -r masks.zip /content/images/masks"]},{"cell_type":"markdown","id":"sI3mNCa-WVc6","metadata":{"id":"sI3mNCa-WVc6"},"source":["## Individual Processing with Gradio"]},{"cell_type":"markdown","id":"7lMdp898VLJ6","metadata":{"id":"7lMdp898VLJ6"},"source":["You can ignore the error regarding pip's dependency when this cell is run."]},{"cell_type":"code","execution_count":5,"id":"N0aaLRzh0X0-","metadata":{"collapsed":true,"executionInfo":{"elapsed":5635,"status":"ok","timestamp":1737895873403,"user":{"displayName":"Kyaw Ye Thu","userId":"06739278510030916737"},"user_tz":-540},"id":"N0aaLRzh0X0-"},"outputs":[],"source":["!pip install -q gradio gradio_image_prompter"]},{"cell_type":"markdown","id":"fCop0thDVSoa","metadata":{"id":"fCop0thDVSoa"},"source":["Launch a Gradio app."]},{"cell_type":"code","execution_count":null,"id":"XJ75EsOvkFFJ","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"XJ75EsOvkFFJ","outputId":"ed834adf-aca1-4188-f0cb-90066158acb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://820c5cdcb5a2329625.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://820c5cdcb5a2329625.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}],"source":["import gradio as gr\n","import gradio_image_prompter as gr_ext\n","from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n","import gc\n","from google.colab import files\n","\n","title = \"Manual Masking with Segment Anything Model (SAM)\"\n","header = (\n","    \"<div align='center'>\"\n","    \"<h1>Manual Masking with Segment Anything Model (SAM)</h1>\"\n","    \"</div>\"\n",")\n","theme = \"soft\"\n","css = \"\"\"#anno-img .mask {opacity: 0.5; transition: all 0.2s ease-in-out;}\n","            #anno-img .mask.active {opacity: 0.7}\"\"\"\n","\n","blur = GaussianBlur(kernel_size=23, sigma=20)\n","\n","def on_click_submit_btn(click_input_img):\n","    predictor.set_image(click_input_img['image'])\n","    np_points = np.array(click_input_img['points'])\n","\n","\n","    # Get only points where the last column ([:, 5]) is 4 (click points)\n","    point_condition = (np_points[:, 5] == 4)\n","    input_points = np_points[point_condition][:, :2]  # Get x,y coordinates\n","\n","    # Get unique coordinates as a list of tuples\n","    unique_tuples = []\n","    for point in input_points:\n","        point_tuple = tuple(point)\n","        if point_tuple not in unique_tuples:\n","            unique_tuples.append(point_tuple)\n","    input_points = np.array(unique_tuples)\n","\n","    # All points are positive points (label = 1)\n","    input_labels = np.ones(len(input_points))\n","\n","    # Get prediction from SAM\n","    masks, _, _ = predictor.predict(\n","        point_coords=input_points,\n","        point_labels=input_labels,\n","        multimask_output=True,\n","    )\n","\n","    # Create a black blank canvas\n","    mask_all = np.zeros((click_input_img['image'].shape[0], click_input_img['image'].shape[1], 3, len(masks)))\n","\n","    # Apply mask\n","    for i in range(len(masks)):\n","      white_mask = (np.array([255, 255, 255]) / 255).tolist()\n","      mask_all[masks[i], :, i] = white_mask\n","\n","    # Convert the NumPy array to a PyTorch tensor to apply GaussianBlur\n","    for i in range(len(masks)):\n","      tensor_image = torch.from_numpy(mask_all[:, :, :, i]).permute(2, 0, 1)  # Change to (C, H, W) for PyTorch\n","      blurred_tensor = blur(tensor_image)\n","      mask_all[:, :, :, i] = blurred_tensor.permute(1, 2, 0).numpy()  # Change back to (H, W, C) NumPy array\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    return mask_all[..., 0], mask_all[..., 1], mask_all[..., 2]\n","\n","def on_click_save_btn(mask):\n","    mask_dir = 'images/masks'\n","    num_files = len([f for f in os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f))])\n","    mask_path = os.path.join(mask_dir, f\"mask_{num_files}.png\")\n","    Image.fromarray(mask).save(mask_path)\n","    files.download(mask_path)\n","    gr.Info(f'Mask successfully saved as {mask_path}. All the masks will be downloaded together once you stop running the cell.', duration=13)\n","\n","    return None\n","\n","with gr.Blocks(title=title, theme=theme, css=css) as demo:\n","    gr.Markdown(header)\n","\n","    gr.Markdown(\"\"\"\n","      Manually select the objects to be masked by clicking on it.      -\n","      - Click `Submit` **after clicking at least once** to receive three different masks.\n","      - Click one of the three buttons (`Mask 1`, `Mask 2`, `Mask 3`) to save and download. Downloading will begin when the cell that initiatied the demo is stopped.\n","      - With multiple people, some faces clicked may not be fully masked. Then, click another point on the face again **without clearing** and `submit` again.\n","      - In general, the mask of `Mask 1`, `Mask 2`, and `Mask 3` becomes increasingly inclusive. For example, while `Mask 1` only masks the face and `Mask 3` masks the whole body given **a coordinate on the face**.\n","        - Therefore, clicking once only on the face is enough to capture other exposed body parts like hands, which is usually generated as `Mask 2`.\n","    \"\"\")\n","    with gr.Row():\n","        with gr.Column():\n","          click_input_img = gr_ext.ImagePrompter(\n","              show_label=True,\n","              label=\"Input Image\",\n","              interactive=True,\n","              sources='upload'\n","          )\n","        with gr.Column():\n","          with gr.Tab(\"Mask 1\"):\n","              output_mask_1 = gr.Image(\n","                  interactive=False,\n","                  show_label=False,\n","                  show_download_button=False\n","                  )\n","          with gr.Tab(\"Mask 2\"):\n","              output_mask_2 = gr.Image(\n","                  show_label=False,\n","                  interactive=False,\n","                  show_download_button=False\n","                  )\n","          with gr.Tab(\"Mask 3\"):\n","              output_mask_3 = gr.Image(\n","                  show_label=False,\n","                  interactive=False,\n","                  show_download_button=False\n","                  )\n","\n","    with gr.Row():\n","            click_save_btn_1 = gr.Button(\"Mask 1\")\n","            click_save_btn_2 = gr.Button(\"Mask 2\")\n","            click_save_btn_3 = gr.Button(\"Mask 3\")\n","\n","    with gr.Row():\n","            click_clr_btn=gr.ClearButton(components=[click_input_img, output_mask_1, output_mask_2, output_mask_3])\n","            click_submit_btn = gr.Button(\"Submit\")\n","\n","    click_submit_btn.click(\n","        fn=on_click_submit_btn,\n","        inputs=[click_input_img],\n","        outputs=[output_mask_1, output_mask_2, output_mask_3]\n","    )\n","\n","    click_save_btn_1.click(\n","        fn=on_click_save_btn,\n","        inputs=[output_mask_1],\n","        outputs=None\n","    )\n","\n","    click_save_btn_2.click(\n","        fn=on_click_save_btn,\n","        inputs=[output_mask_2],\n","        outputs=None\n","    )\n","\n","    click_save_btn_3.click(\n","        fn=on_click_save_btn,\n","        inputs=[output_mask_3],\n","        outputs=None\n","    )\n","\n","\n","if __name__ == \"__main__\":\n","    demo.launch(debug=True)"]},{"cell_type":"code","execution_count":null,"id":"VHArdWVUKQil","metadata":{"id":"VHArdWVUKQil"},"outputs":[],"source":["!zip -r masks.zip /content/images/masks"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["644532a8","0be845da"],"gpuType":"T4","provenance":[{"file_id":"https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb","timestamp":1726541225600}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}