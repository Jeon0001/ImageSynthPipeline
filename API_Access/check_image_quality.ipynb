{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4ZqHMbQs2x"
      },
      "source": [
        "# Checking the Quality of Synthesized Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-lKF1JTuVLr"
      },
      "source": [
        "To automatically evaluate the structural integrity of augmentations in the image while retaining resemblance to the original,\n",
        "- We use **BRISQUE**, a reference-free metric\n",
        "which quantifies the perceptual quality of an image, labeling images with a score under 70 as highly\n",
        "salient.\n",
        "- Similarly, we use **CLIP** similarity between original and augmented images to ensure the diffusion model performed substantial enough augmentations on the original.\n",
        "\n",
        "> ***Currently, the thresholds are arbitarily defined. We can discuss later to set them.***\n",
        "\n",
        "> Nvidia GPU is needed to run this notebook. If not, recommended to run it on Google Colab. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bjf_09KLvF4Ml2pA-jLAX2E-pAHIm7HO)\n",
        "\n",
        "Reference: [Semi-Truths: A Large-Scale Dataset of AI-Augmented\n",
        "Images for Evaluating Robustness of AI-Generated\n",
        "Image detectors](https://arxiv.org/pdf/2411.07472)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1hHhakcXetc"
      },
      "source": [
        "## Environmental Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3f81s1jGcVw",
        "outputId": "f98a5a12-6d9f-4f6b-f12c-677bdd6004cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/140.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for libsvm-official (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q brisque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R1GgMT8ABBQ8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pdb\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer, CLIPImageProcessor\n",
        "from PIL import Image, ImageFile\n",
        "from brisque import BRISQUE\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTkZvlgvA-QM"
      },
      "outputs": [],
      "source": [
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
        "\n",
        "model_ID = \"openai/clip-vit-base-patch32\"\n",
        "model = CLIPModel.from_pretrained(model_ID)\n",
        "model.to(torch_device)\n",
        "\n",
        "tokenizer = CLIPTokenizer.from_pretrained(model_ID)\n",
        "preprocess = CLIPImageProcessor.from_pretrained(model_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOyvUNyWBpWw"
      },
      "source": [
        "## Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B_0LHOxwBP5B"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    \"\"\"\n",
        "    Preprocess image using transformers.ClipImageProcessor.\n",
        "    \"\"\"\n",
        "    image = preprocess(image, return_tensors=\"pt\")\n",
        "    return image\n",
        "\n",
        "\n",
        "def calculate_image_similarity(image1, image2):\n",
        "    image1 = preprocess_image(image1)[\"pixel_values\"].to(torch_device)\n",
        "    image2 = preprocess_image(image2)[\"pixel_values\"].to(torch_device)\n",
        "    image_encoding1 = model.get_image_features(image1)\n",
        "    image_encoding2 = model.get_image_features(image2)\n",
        "    similarity = torch.nn.functional.cosine_similarity(\n",
        "        image_encoding1, image_encoding2, dim=-1\n",
        "    )\n",
        "    return similarity.item()\n",
        "\n",
        "\n",
        "def brisque_Score(img):\n",
        "    \"\"\"\n",
        "    Computes Brisque score for an image.\n",
        "    Leveraging the brisque[openvs-python] library.\n",
        "\n",
        "    Inputs:\n",
        "    ----------------\n",
        "    img : PIL.Image\n",
        "        Input image.\n",
        "\n",
        "    Returns:\n",
        "    ----------------\n",
        "    score : float\n",
        "        Brisque score.\n",
        "    \"\"\"\n",
        "    ndarray = np.asarray(img)\n",
        "    obj = BRISQUE(url=False)\n",
        "    score = obj.score(img=ndarray)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAzPEze7r6sf"
      },
      "source": [
        "## Define Main Functions and Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "WEverelvQe3g"
      },
      "outputs": [],
      "source": [
        "def process_images(original_img_dir, synthesized_img_dir, output_csv_file):\n",
        "    \"\"\"\n",
        "    Given an original image directory and synthesized image directory (containing multiple races),\n",
        "    calculate quality scores for each synthesized image and save the results to a CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for race_dir in os.listdir(synthesized_img_dir):\n",
        "        for filename in os.listdir(original_img_dir):\n",
        "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                original_img_path = os.path.join(original_img_dir, filename)\n",
        "\n",
        "                filename_parts = filename.split('_')\n",
        "                filename_parts.insert(1, race_dir)\n",
        "                synthesized_filename = '_'.join(filename_parts)\n",
        "                synthesized_path = os.path.join(synthesized_img_dir, race_dir, synthesized_filename)\n",
        "\n",
        "                try:\n",
        "                    orig_img = Image.open(original_img_path)\n",
        "                    synthesized_img = Image.open(synthesized_path)\n",
        "\n",
        "                    img_similarity = round(calculate_image_similarity(orig_img, synthesized_img),2)\n",
        "                    brisque_score = round(brisque_Score(synthesized_img),2)\n",
        "\n",
        "                    data.append([synthesized_filename, img_similarity, brisque_score])\n",
        "                except FileNotFoundError:\n",
        "                    print(f\"Warning: Synthesized image not found for {synthesized_filename}. Skipping.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {synthesized_filename}: {e}\")\n",
        "        print(f\"Scores for images in {race_dir} calculated.\")\n",
        "\n",
        "    processed_df = pd.DataFrame(data, columns=['filename', 'img_similarity', 'brisque_score'])\n",
        "\n",
        "    processed_df.to_csv(output_csv_file, index=False)\n",
        "    print(f\"Results saved to {output_csv_file}\")\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "def filter_df(df, metric_thresholds):\n",
        "    # select rows that don't pass the criteria\n",
        "    similarity_threshold = metric_thresholds['img_similarity']\n",
        "    brisque_threshold = metric_thresholds['brisque_score']\n",
        "\n",
        "    failed_df = df[\n",
        "        (df['img_similarity'] < similarity_threshold[0]) |\n",
        "        (df['img_similarity'] > similarity_threshold[1]) |\n",
        "        (df['brisque_score'] < brisque_threshold[0]) |\n",
        "        (df['brisque_score'] > brisque_threshold[1])\n",
        "         ]\n",
        "    return failed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Psqe5I3cXnTY",
        "outputId": "b75c6969-b4e9-4791-94a4-300061712dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scores for images in Black calculated.\n",
            "Scores for images in Indian calculated.\n",
            "Scores for images in White calculated.\n",
            "Scores for images in Asian calculated.\n",
            "Results saved to Myanmar_Food.csv\n"
          ]
        }
      ],
      "source": [
        "original_img_dir = 'drive/MyDrive/Team VQA - Datasets/Myanmar_Food/original_images/'\n",
        "synthesized_img_dir = 'drive/MyDrive/Team VQA - Datasets/Myanmar_Food/synthesized_images/'\n",
        "output_csv_file = 'Myanmar_Food.csv'\n",
        "metric_thresholds = {\n",
        "    \"img_similarity\": [0.72, 0.9896],\n",
        "    \"brisque_score\": [0, 75],\n",
        "}\n",
        "\n",
        "processed_df = process_images(original_img_dir, synthesized_img_dir, output_csv_file)\n",
        "failed_df = filter_df(processed_df, metric_thresholds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "J1hHhakcXetc",
        "oOyvUNyWBpWw"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
