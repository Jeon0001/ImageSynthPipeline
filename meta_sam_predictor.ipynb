{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ae39ff",
   "metadata": {
    "id": "a1ae39ff"
   },
   "source": [
    "# Object masks from prompts with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4b25c",
   "metadata": {
    "id": "b4a4b25c"
   },
   "source": [
    "\n",
    "To run in Google Colab [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1A0LpGaDId6-7_okw47YXkqoXNoetHVp8) [**recommended**]\n",
    "\n",
    "The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644532a8",
   "metadata": {
    "id": "644532a8"
   },
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aL7AQn2E5PS",
   "metadata": {
    "executionInfo": {
     "elapsed": 19355,
     "status": "ok",
     "timestamp": 1736782850257,
     "user": {
      "displayName": "Kyaw Ye Thu",
      "userId": "15265666706494287045"
     },
     "user_tz": -540
    },
    "id": "6aL7AQn2E5PS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import io\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "import sys\n",
    "from torchvision.transforms import GaussianBlur\n",
    "from tqdm import tqdm\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd9a89",
   "metadata": {
    "collapsed": true,
    "id": "91dd9a89"
   },
   "outputs": [],
   "source": [
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "!{sys.executable} -m pip install opencv-python\n",
    "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "!pip install ipympl\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "face_detection_model = \"face_detection_yunet_2023mar.onnx\"\n",
    "\n",
    "if not os.path.isdir('models'):\n",
    "  !mkdir models\n",
    "# download SAM\n",
    "  if not os.path.isfile(f'models/{sam_checkpoint}'):\n",
    "    !wget https://dl.fbaipublicfiles.com/segment_anything/$sam_checkpoint\n",
    "    !mv $sam_checkpoint models/\n",
    "\n",
    "  # download YuNet Faec Detection Model\n",
    "  if not os.path.isfile(f'models/{face_detection_model}'):\n",
    "    !wget https://github.com/astaileyyoung/CineFace/raw/main/research/data/face_detection_yunet_2023mar.onnx\n",
    "    !mv $face_detection_model models/\n",
    "\n",
    "if not os.path.isdir('images/original_images'):\n",
    "  !mkdir -p images/original_images\n",
    "\n",
    "if not os.path.isdir('images/masks'):\n",
    "  !mkdir -p images/masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be845da",
   "metadata": {
    "id": "0be845da"
   },
   "source": [
    "## Program Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1927b",
   "metadata": {
    "id": "0bb1927b"
   },
   "source": [
    "Load the SAM model and predictor. Running on CUDA and using the default model are recommended for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e28150b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15644,
     "status": "ok",
     "timestamp": 1736782911967,
     "user": {
      "displayName": "Kyaw Ye Thu",
      "userId": "15265666706494287045"
     },
     "user_tz": -540
    },
    "id": "7e28150b",
    "outputId": "d8ff26a7-e516-46d7-8548-62f543965158"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "model_type = \"vit_h\"\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=f'models/{sam_checkpoint}')\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33681dd1",
   "metadata": {
    "id": "33681dd1"
   },
   "source": [
    "Face detector class, `FaceDetectorYuNet()`, and helper functions for displaying points, boxes, and masks are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29bc90d5",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1736782911968,
     "user": {
      "displayName": "Kyaw Ye Thu",
      "userId": "15265666706494287045"
     },
     "user_tz": -540
    },
    "id": "29bc90d5"
   },
   "outputs": [],
   "source": [
    "class FaceDetectorYunet():\n",
    "    def __init__(self,\n",
    "                  model_path='models/face_detection_yunet_2023mar.onnx',\n",
    "                  img_size=(300, 300),\n",
    "                  threshold=0.5):\n",
    "        self.model_path = model_path\n",
    "        self.img_size = img_size\n",
    "        self.fd = cv2.FaceDetectorYN_create(str(model_path),\n",
    "                                            \"\",\n",
    "                                            img_size,\n",
    "                                            score_threshold=threshold)\n",
    "\n",
    "    def draw_faces(self,\n",
    "                   image,\n",
    "                   faces,\n",
    "                   show_confidence=False):\n",
    "        for face in faces:\n",
    "            color = (0, 0, 255)\n",
    "            thickness = 2\n",
    "            cv2.rectangle(image, (face['x1'], face['y1']), (face['x2'], face['y2']), color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            if show_confidence:\n",
    "                confidence = face['confidence']\n",
    "                confidence = \"{:.2f}\".format(confidence)\n",
    "                position = (face['x1'], face['y1'] - 10)\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                scale = 0.5\n",
    "                thickness = 1\n",
    "                cv2.putText(image, confidence, position, font, scale, color, thickness, cv2.LINE_AA)\n",
    "        return image\n",
    "\n",
    "    def scale_coords(self, image, prediction):\n",
    "        ih, iw = image.shape[:2]\n",
    "        rw, rh = self.img_size\n",
    "        a = np.array([\n",
    "                (prediction['x1'], prediction['y1']),\n",
    "                (prediction['x1'] + prediction['x2'], prediction['y1'] + prediction['y2'])\n",
    "                    ])\n",
    "        b = np.array([iw/rw, ih/rh])\n",
    "        c = a * b\n",
    "        prediction['img_width'] = iw\n",
    "        prediction['img_height'] = ih\n",
    "        prediction['x1'] = int(c[0,0].round())\n",
    "        prediction['x2'] = int(c[1,0].round())\n",
    "        prediction['y1'] = int(c[0,1].round())\n",
    "        prediction['y2'] = int(c[1,1].round())\n",
    "        prediction['face_width'] = (c[1,0] - c[0,0])\n",
    "        prediction['face_height'] = (c[1,1] - c[0,1])\n",
    "        # prediction['face_width'] = prediction['x2'] - prediction['x1']\n",
    "        # prediction['face_height'] = prediction['y2'] - prediction['y1']\n",
    "        prediction['area'] = prediction['face_width'] * prediction['face_height']\n",
    "        prediction['pct_of_frame'] = prediction['area']/(prediction['img_width'] * prediction['img_height'])\n",
    "        return prediction\n",
    "\n",
    "    def detect(self, image):\n",
    "        if isinstance(image, str):\n",
    "            image = cv2.imread(str(image))\n",
    "        img = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
    "        img = cv2.resize(img, self.img_size)\n",
    "        self.fd.setInputSize(self.img_size)\n",
    "        _, faces = self.fd.detect(img)\n",
    "        if faces is None:\n",
    "            return None\n",
    "        else:\n",
    "            predictions = self.parse_predictions(image, faces)\n",
    "            return predictions\n",
    "\n",
    "    def parse_predictions(self,\n",
    "                          image,\n",
    "                          faces):\n",
    "        data = []\n",
    "        for num, face in enumerate(list(faces)):\n",
    "            x1, y1, x2, y2 = list(map(int, face[:4]))\n",
    "            landmarks = list(map(int, face[4:len(face)-1]))\n",
    "            landmarks = np.array_split(landmarks, len(landmarks) / 2)\n",
    "            positions = ['left_eye', 'right_eye', 'nose', 'right_mouth', 'left_mouth']\n",
    "            landmarks = {positions[num]: x.tolist() for num, x in enumerate(landmarks)}\n",
    "            confidence = face[-1]\n",
    "            datum = {'x1': x1,\n",
    "                     'y1': y1,\n",
    "                     'x2': x2,\n",
    "                     'y2': y2,\n",
    "                     'face_num': num,\n",
    "                     'landmarks': landmarks,\n",
    "                     'confidence': confidence,\n",
    "                     'model': 'yunet'}\n",
    "            d = self.scale_coords(image, datum)\n",
    "            data.append(d)\n",
    "        return data\n",
    "\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, display=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        # color to inpaint (white)\n",
    "        color = np.array([255, 255, 255])\n",
    "\n",
    "    # color to keep intact (black)\n",
    "    background_color = np.array([0, 0, 0])\n",
    "    h, w = mask.shape[-2:]\n",
    "\n",
    "    # Reshape the mask to have the same number of color channels\n",
    "    mask = mask.reshape(h, w, 1)\n",
    "\n",
    "    # Apply color to the mask where mask is True, and color2 where mask is False\n",
    "    mask_image = mask * color.reshape(1, 1, -1) + ~mask * background_color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kbwlMXRtn_tj",
   "metadata": {
    "id": "kbwlMXRtn_tj"
   },
   "source": [
    "# Batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LSybFHjpIp-9",
   "metadata": {
    "id": "LSybFHjpIp-9"
   },
   "source": [
    "Extract the compressed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bSkoPbNQSPCo",
   "metadata": {
    "id": "bSkoPbNQSPCo"
   },
   "outputs": [],
   "source": [
    "# !unzip ./food.zip -d ./images/input_images/\n",
    "# !rm -rf ./food.zip\n",
    "\n",
    "!tar -xf original_images.tar.xz\n",
    "!mv original_images/* images/original_images/\n",
    "!rm -rf original_images/\n",
    "!rm -rf ./original_images.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l5qaMUUgU9M5",
   "metadata": {
    "id": "l5qaMUUgU9M5"
   },
   "source": [
    "Perform batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hnJTSkX_n2_t",
   "metadata": {
    "collapsed": true,
    "id": "hnJTSkX_n2_t"
   },
   "outputs": [],
   "source": [
    "def batch_detect_faces(face_detector, input_img_dir, output_mask_dir, max_num_faces=2, save_boxes=False):\n",
    "    # Create necessary directories if required\n",
    "    if save_boxes: os.makedirs('images/box_images', exist_ok=True)\n",
    "\n",
    "    # Loop through all images in the directory\n",
    "    for image_path in tqdm(os.listdir(input_img_dir)):\n",
    "        full_image_path = os.path.join(input_img_dir, image_path)\n",
    "\n",
    "        # Skip non-image files (e.g., .DS_Store, other system files)\n",
    "        if image_path.startswith('.') or not image_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            print(f\"Skipping non-image file: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        # Load the image\n",
    "        img = cv2.imread(full_image_path)\n",
    "\n",
    "        # Check if the image was loaded successfully\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image: {full_image_path}\")\n",
    "            continue\n",
    "\n",
    "        faces = face_detector.detect(img)\n",
    "\n",
    "        # If no faces are detected, skip to the next image\n",
    "        if not faces:\n",
    "            print(f\"No face detected in {image_path}.\")\n",
    "            continue\n",
    "\n",
    "        # Remove faces with low confidence\n",
    "        for face in faces:\n",
    "          if face['confidence'] < 0.55:\n",
    "            faces.remove(face)\n",
    "\n",
    "        # Only select faces with max_num_faces highest confidence at most\n",
    "        faces = sorted(faces, key=lambda x: x['confidence'], reverse=True)[:max_num_faces]\n",
    "\n",
    "        # If faces become empty after filtering\n",
    "        if not faces:\n",
    "            print(f\"No eligible faces detected in {image_path}.\")\n",
    "            continue\n",
    "\n",
    "        # Prepare face center coordinates and labels\n",
    "        face_centers = [((face['x1'] + face['x2']) / 2, (face['y1'] + face['y2']) / 2) for face in faces]\n",
    "        input_point = np.array(face_centers)\n",
    "        input_label = np.ones(len(input_point))\n",
    "\n",
    "        # Optionally save boxes around faces\n",
    "        if save_boxes:\n",
    "            box_file_name = os.path.splitext(os.path.basename(image_path))[0] + \"_box.png\"\n",
    "            img_copy = img.copy()\n",
    "            face_detector.draw_faces(img_copy, faces, show_confidence=True)\n",
    "            cv2.imwrite(f'images/box_images/{box_file_name}', img_copy)\n",
    "\n",
    "        # Process image and make predictions\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        predictor.set_image(img)\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=input_point,\n",
    "            point_labels=input_label,\n",
    "            multimask_output=True,\n",
    "        )\n",
    "\n",
    "        # Select the second mask, which usually includes just face and body parts\n",
    "        mask_index = 1\n",
    "        mask_input = logits[mask_index, :, :]\n",
    "    \n",
    "        # Using the second mask, creates a more accurate mask\n",
    "        masks, _, _ = predictor.predict(\n",
    "            point_coords=input_point,\n",
    "            point_labels=input_label,\n",
    "            mask_input=mask_input[None, :, :],\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        # Create a mask image\n",
    "        mask_file_name = os.path.splitext(os.path.basename(image_path))[0] + \"_mask.png\"\n",
    "        save_mask_path = os.path.join(output_mask_dir, mask_file_name)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        show_mask(masks, plt.gca())\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Save the mask with blur applied\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', bbox_inches='tight')\n",
    "        buf.seek(0)\n",
    "        plt.close()\n",
    "        blur = GaussianBlur(kernel_size=25, sigma=20)\n",
    "        blurred_mask = blur(Image.open(buf))\n",
    "        blurred_mask.save(save_mask_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baFAavYsEXV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89132,
     "status": "ok",
     "timestamp": 1736766132471,
     "user": {
      "displayName": "Kyaw Ye Thu",
      "userId": "06739278510030916737"
     },
     "user_tz": -540
    },
    "id": "1baFAavYsEXV",
    "outputId": "b472e1a7-2965-4d52-87eb-1e287a788024"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 12/34 [00:32<00:59,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-image file: .ipynb_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 18/34 [00:46<00:40,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected in Myanmar_food_21.png.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [01:28<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "face_detector = FaceDetectorYunet()\n",
    "input_img_dir = '/content/images/original_images/'\n",
    "output_mask_dir = '/content/images/masks/'\n",
    "batch_detect_faces(face_detector, input_img_dir, output_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gIGc9aRPUkbs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1736766149440,
     "user": {
      "displayName": "Kyaw Ye Thu",
      "userId": "06739278510030916737"
     },
     "user_tz": -540
    },
    "id": "gIGc9aRPUkbs",
    "outputId": "afd91d90-e3ec-423e-c4ce-4b928cee4cfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/images/masks/ (stored 0%)\n",
      "  adding: content/images/masks/Myanmar_food_31_mask.png (deflated 13%)\n",
      "  adding: content/images/masks/Myanmar_food_18_mask.png (deflated 11%)\n",
      "  adding: content/images/masks/Myanmar_festival_12_mask.png (deflated 6%)\n",
      "  adding: content/images/masks/Myanmar_food_26_mask.png (deflated 29%)\n",
      "  adding: content/images/masks/Myanmar_festival_22_mask.png (deflated 6%)\n",
      "  adding: content/images/masks/Myanmar_food_4_mask.png (deflated 15%)\n",
      "  adding: content/images/masks/Myanmar_food_8_mask.png (deflated 4%)\n",
      "  adding: content/images/masks/Myanmar_food_1_mask.png (deflated 15%)\n",
      "  adding: content/images/masks/Myanmar_food_10_mask.png (deflated 8%)\n",
      "  adding: content/images/masks/Myanmar_food_23_mask.png (deflated 22%)\n",
      "  adding: content/images/masks/Myanmar_festival_31_mask.png (deflated 5%)\n",
      "  adding: content/images/masks/Myanmar_food_3_mask.png (deflated 21%)\n",
      "  adding: content/images/masks/Myanmar_festival_8_mask.png (deflated 5%)\n",
      "  adding: content/images/masks/Myanmar_festival_20_mask.png (deflated 4%)\n",
      "  adding: content/images/masks/Myanmar_food_29_mask.png (deflated 16%)\n",
      "  adding: content/images/masks/Myanmar_festival_2_mask.png (deflated 7%)\n",
      "  adding: content/images/masks/Myanmar_festival_25_mask.png (deflated 4%)\n",
      "  adding: content/images/masks/Myanmar_food_24_mask.png (deflated 29%)\n",
      "  adding: content/images/masks/Myanmar_festival_15_mask.png (deflated 7%)\n",
      "  adding: content/images/masks/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: content/images/masks/Myanmar_festival_24_mask.png (deflated 7%)\n",
      "  adding: content/images/masks/Myanmar_festival_19_mask.png (deflated 3%)\n",
      "  adding: content/images/masks/Myanmar_food_27_mask.png (deflated 10%)\n",
      "  adding: content/images/masks/Myanmar_food_12_mask.png (deflated 16%)\n",
      "  adding: content/images/masks/Myanmar_festival_6_mask.png (deflated 3%)\n",
      "  adding: content/images/masks/Myanmar_food_22_mask.png (deflated 30%)\n",
      "  adding: content/images/masks/Myanmar_food_5_mask.png (deflated 10%)\n",
      "  adding: content/images/masks/Myanmar_food_14_mask.png (deflated 3%)\n",
      "  adding: content/images/masks/Myanmar_festival_27_mask.png (deflated 14%)\n",
      "  adding: content/images/masks/Myanmar_food_25_mask.png (deflated 18%)\n",
      "  adding: content/images/masks/Myanmar_food_7_mask.png (deflated 13%)\n",
      "  adding: content/images/masks/Myanmar_festival_0_mask.png (deflated 8%)\n",
      "  adding: content/images/masks/Myanmar_food_11_mask.png (deflated 7%)\n",
      "  adding: content/images/masks/Myanmar_food_9_mask.png (deflated 6%)\n",
      "  adding: content/images/masks/Myanmar_festival_30_mask.png (deflated 26%)\n",
      "  adding: content/images/masks/Myanmar_festival_16_mask.png (deflated 45%)\n",
      "  adding: content/images/masks/Myanmar_festival_32_mask.png (deflated 8%)\n",
      "  adding: content/images/masks/Myanmar_festival_18_mask.png (deflated 2%)\n",
      "  adding: content/images/masks/Myanmar_festival_4_mask.png (deflated 2%)\n",
      "  adding: content/images/masks/Myanmar_festival_10_mask.png (deflated 7%)\n",
      "  adding: content/images/masks/Myanmar_food_19_mask.png (deflated 13%)\n",
      "  adding: content/images/masks/Myanmar_festival_17_mask.png (deflated 6%)\n",
      "  adding: content/images/masks/Myanmar_festival_1_mask.png (deflated 43%)\n",
      "  adding: content/images/masks/Myanmar_food_16_mask.png (deflated 12%)\n",
      "  adding: content/images/masks/Myanmar_food_32_mask.png (deflated 27%)\n",
      "  adding: content/images/masks/Myanmar_festival_5_mask.png (deflated 2%)\n",
      "  adding: content/images/masks/mask_0.png (deflated 15%)\n",
      "  adding: content/images/masks/Myanmar_food_6_mask.png (deflated 37%)\n",
      "  adding: content/images/masks/Myanmar_festival_21_mask.png (deflated 10%)\n",
      "  adding: content/images/masks/Myanmar_food_30_mask.png (deflated 20%)\n",
      "  adding: content/images/masks/Myanmar_food_15_mask.png (deflated 9%)\n",
      "  adding: content/images/masks/Myanmar_food_13_mask.png (deflated 9%)\n",
      "  adding: content/images/masks/Myanmar_festival_28_mask.png (deflated 3%)\n",
      "  adding: content/images/masks/Myanmar_food_28_mask.png (deflated 7%)\n",
      "  adding: content/images/masks/Myanmar_food_2_mask.png (deflated 13%)\n",
      "  adding: content/images/masks/mask_2.png (deflated 18%)\n",
      "  adding: content/images/masks/Myanmar_festival_29_mask.png (deflated 7%)\n",
      "  adding: content/images/masks/Myanmar_food_17_mask.png (deflated 7%)\n",
      "  adding: content/images/masks/Myanmar_festival_11_mask.png (deflated 7%)\n",
      "  adding: content/images/masks/mask_1.png (deflated 17%)\n",
      "  adding: content/images/masks/Myanmar_food_20_mask.png (deflated 9%)\n",
      "  adding: content/images/masks/Myanmar_festival_7_mask.png (deflated 3%)\n",
      "  adding: content/images/masks/Myanmar_food_0_mask.png (deflated 14%)\n",
      "  adding: content/images/masks/Myanmar_festival_13_mask.png (deflated 2%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r masks.zip /content/images/masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sI3mNCa-WVc6",
   "metadata": {
    "id": "sI3mNCa-WVc6"
   },
   "source": [
    "# Individual Processing with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7lMdp898VLJ6",
   "metadata": {
    "id": "7lMdp898VLJ6"
   },
   "source": [
    "You can ignore the error regarding pip's dependency when this cell is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N0aaLRzh0X0-",
   "metadata": {
    "collapsed": true,
    "id": "N0aaLRzh0X0-"
   },
   "outputs": [],
   "source": [
    "!pip install -q gradio gradio_image_prompter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fCop0thDVSoa",
   "metadata": {
    "id": "fCop0thDVSoa"
   },
   "source": [
    "Launch a Gradio app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XJ75EsOvkFFJ",
   "metadata": {
    "id": "XJ75EsOvkFFJ"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import gradio_image_prompter as gr_ext\n",
    "from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n",
    "import gc\n",
    "from google.colab import files\n",
    "\n",
    "title = \"Manual Masking with Segment Anything Model (SAM)\"\n",
    "header = (\n",
    "    \"<div align='center'>\"\n",
    "    \"<h1>Manual Masking with Segment Anything Model (SAM)</h1>\"\n",
    "    \"</div>\"\n",
    ")\n",
    "theme = \"soft\"\n",
    "css = \"\"\"#anno-img .mask {opacity: 0.5; transition: all 0.2s ease-in-out;}\n",
    "            #anno-img .mask.active {opacity: 0.7}\"\"\"\n",
    "\n",
    "blur = GaussianBlur(kernel_size=25, sigma=20)\n",
    "\n",
    "def on_click_submit_btn(click_input_img):\n",
    "    predictor.set_image(click_input_img['image'])\n",
    "    np_points = np.array(click_input_img['points'])\n",
    "\n",
    "\n",
    "    # Get only points where the last column ([:, 5]) is 4 (click points)\n",
    "    point_condition = (np_points[:, 5] == 4)\n",
    "    input_points = np_points[point_condition][:, :2]  # Get x,y coordinates\n",
    "\n",
    "    # Get unique coordinates as a list of tuples\n",
    "    unique_tuples = []\n",
    "    for point in input_points:\n",
    "        point_tuple = tuple(point)\n",
    "        if point_tuple not in unique_tuples:\n",
    "            unique_tuples.append(point_tuple)\n",
    "    input_points = np.array(unique_tuples)\n",
    "\n",
    "    # All points are positive points (label = 1)\n",
    "    input_labels = np.ones(len(input_points))\n",
    "\n",
    "    # Get prediction from SAM\n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=input_points,\n",
    "        point_labels=input_labels,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "\n",
    "    # Create a black blank canvas\n",
    "    mask_all = np.zeros((click_input_img['image'].shape[0], click_input_img['image'].shape[1], 3, len(masks)))\n",
    "\n",
    "    # Apply mask\n",
    "    for i in range(len(masks)):\n",
    "      white_mask = (np.array([255, 255, 255]) / 255).tolist()\n",
    "      mask_all[masks[i], :, i] = white_mask\n",
    "\n",
    "    # Convert the NumPy array to a PyTorch tensor to apply GaussianBlur\n",
    "    for i in range(len(masks)):\n",
    "      tensor_image = torch.from_numpy(mask_all[:, :, :, i]).permute(2, 0, 1)  # Change to (C, H, W) for PyTorch\n",
    "      blurred_tensor = blur(tensor_image)\n",
    "      mask_all[:, :, :, i] = blurred_tensor.permute(1, 2, 0).numpy()  # Change back to (H, W, C) NumPy array\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return mask_all[..., 0], mask_all[..., 1], mask_all[..., 2]\n",
    "\n",
    "def on_click_save_btn(mask):\n",
    "    mask_dir = 'images/masks'\n",
    "    num_files = len([f for f in os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f))])\n",
    "    mask_path = os.path.join(mask_dir, f\"mask_{num_files}.png\")\n",
    "    Image.fromarray(mask).save(mask_path)\n",
    "    files.download(mask_path)\n",
    "    gr.Info(f'Mask successfully saved as {mask_path}. All the masks will be downloaded together once you stop running the cell.', duration=13)\n",
    "\n",
    "    return None\n",
    "\n",
    "with gr.Blocks(title=title, theme=theme, css=css) as demo:\n",
    "    gr.Markdown(header)\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "      Manually select the objects to be masked by clicking on it.\n",
    "      Especially with multiple people, some faces clicked may not be fully masked. Providing more coordinates on such faces is helpful in these cases.\n",
    "\n",
    "      - Click `Submit` **after clicking at least once** to receive three different masks for the input image.\n",
    "      - Then, click one of the three buttons (`Mask 1`, `Mask 2`, `Mask 3`) to save and download. Downloading will begin when the cell that initiatied the demo is stopped.\n",
    "    \"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "          click_input_img = gr_ext.ImagePrompter(\n",
    "              show_label=True,\n",
    "              label=\"Input Image\",\n",
    "              interactive=True,\n",
    "              sources='upload'\n",
    "          )\n",
    "        with gr.Column():\n",
    "          with gr.Tab(\"Mask 1\"):\n",
    "              output_mask_1 = gr.Image(\n",
    "                  interactive=False,\n",
    "                  show_label=False,\n",
    "                  show_download_button=False\n",
    "                  )\n",
    "          with gr.Tab(\"Mask 2\"):\n",
    "              output_mask_2 = gr.Image(\n",
    "                  show_label=False,\n",
    "                  interactive=False,\n",
    "                  show_download_button=False\n",
    "                  )\n",
    "          with gr.Tab(\"Mask 3\"):\n",
    "              output_mask_3 = gr.Image(\n",
    "                  show_label=False,\n",
    "                  interactive=False,\n",
    "                  show_download_button=False\n",
    "                  )\n",
    "\n",
    "    with gr.Row():\n",
    "            click_save_btn_1 = gr.Button(\"Mask 1\")\n",
    "            click_save_btn_2 = gr.Button(\"Mask 2\")\n",
    "            click_save_btn_3 = gr.Button(\"Mask 3\")\n",
    "\n",
    "    with gr.Row():\n",
    "            click_clr_btn=gr.ClearButton(components=[click_input_img, output_mask_1, output_mask_2, output_mask_3])\n",
    "            click_submit_btn = gr.Button(\"Submit\")\n",
    "\n",
    "    click_submit_btn.click(\n",
    "        fn=on_click_submit_btn,\n",
    "        inputs=[click_input_img],\n",
    "        outputs=[output_mask_1, output_mask_2, output_mask_3]\n",
    "    )\n",
    "\n",
    "    click_save_btn_1.click(\n",
    "        fn=on_click_save_btn,\n",
    "        inputs=[output_mask_1],\n",
    "        outputs=None\n",
    "    )\n",
    "\n",
    "    click_save_btn_2.click(\n",
    "        fn=on_click_save_btn,\n",
    "        inputs=[output_mask_2],\n",
    "        outputs=None\n",
    "    )\n",
    "\n",
    "    click_save_btn_3.click(\n",
    "        fn=on_click_save_btn,\n",
    "        inputs=[output_mask_3],\n",
    "        outputs=None\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60451d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian_blur(input_dir, output_dir, kernel_size=25, sigma=20):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Loop through all images in the directory\n",
    "    for image_name in os.listdir(input_dir):\n",
    "        full_image_path = os.path.join(input_dir, image_name)\n",
    "\n",
    "        # Skip non-image files (e.g., .DS_Store, other system files)\n",
    "        if image_name.startswith('.') or not image_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            print(f\"Skipping non-image file: {image_name}\")\n",
    "            continue\n",
    "\n",
    "        # Load the image\n",
    "        img = Image.open(full_image_path)\n",
    "\n",
    "        # Apply Gaussian blur\n",
    "        blur = GaussianBlur(kernel_size, sigma)\n",
    "        blurred_img = blur(img)\n",
    "\n",
    "        # Save the blurred image\n",
    "        output_image_path = os.path.join(output_dir, image_name)\n",
    "        blurred_img.save(output_image_path)\n",
    "\n",
    "# Example usage\n",
    "input_dir = 'images/Azerbaijani_Festivals/masks/'\n",
    "output_dir = 'images/Azerbaijani_Festivals/blurred_masks/'\n",
    "apply_gaussian_blur(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc6a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0be845da",
    "kbwlMXRtn_tj"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb",
     "timestamp": 1726541225600
    }
   ]
  },
  "kernelspec": {
   "display_name": "stability-inpaint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
